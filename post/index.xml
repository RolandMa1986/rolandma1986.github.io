<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Roland&#39;s Blog</title>
    <link>/post/</link>
    <description>Recent content in Posts on Roland&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>This site is licensed under a [Creative Commons Attribution 4.0 International license](https://creativecommons.org/licenses/by-sa/4.0/).</copyright>
    <lastBuildDate>Mon, 24 May 2021 06:52:16 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Helm Tips</title>
      <link>/post/helm-tips/</link>
      <pubDate>Mon, 24 May 2021 06:52:16 +0000</pubDate>
      
      <guid>/post/helm-tips/</guid>
      <description>Helm 是最常用的 Kubernetes 包管理器之一。但实际应使用时却会面对各种复杂的问题，以下内容是对常见应用场景的简要总结：
将 kubectl 部署的资源转为 Helm 资源 如果您的项目最初不是使用 Helm 进行资源管理且已经部署在线上运行，那么你可能需要将已有资源转换 Helm 资源。 否则直接运行 helm install 这时你将看见如下错误信息：
Error: rendered manifests contain a resource that already exists. Unable to continue with install: 
解决这个问题的方法很简单，只需要将对应的资源打上 Helm 的 Label 和 Annotation 即可:
KIND=deployment NAME=my-app-staging RELEASE=staging NAMESPACE=default kubectl annotate $KIND $NAME meta.helm.sh/release-name=$RELEASE kubectl annotate $KIND $NAME meta.</description>
    </item>
    
    <item>
      <title>Kubernetes CNI 插件性能测试</title>
      <link>/post/kubernetes-cni-performance/</link>
      <pubDate>Wed, 07 Apr 2021 01:13:57 +0000</pubDate>
      
      <guid>/post/kubernetes-cni-performance/</guid>
      <description>网络性能是影响 Kubernetes 集群与应用性能的重要指标之一。同时网络架构也是 Kubernetes 中较为复杂模块之一，Kubernetes 通过开放 CNI 接口，实现了网络插件即插即用功能，CNI 插件的实现方式是影响网络性能的最主要因素。目前主流的 CNI 插件包括：Flannel、Calico、Weave、Canal 和 Cilium 等。除 CNI 插件外，Kube-proxy 运行模式也是影响网络性能的重要因素之一。 Kubernetes 通过 Kube-proxy 提供了集群内负载均衡的能力，其内置方式有 Iptables 和 IPVS 等两种。在不同的场景下，CNI 和 Kube-proxy 都会对网络性能产生显著的影响。本文着重对 Kubernetes 网络性能测试的方法以及影响因素进行讲解和分析，不包含具体插件的性能比较。
Kubernetes 网络通讯模式 在 Kubernetes 网络中，主要有以下几种通讯场景：
 容器与容器之间的直接通信，通过本地回环实现，不通过网络插件协议栈，因此测试可以忽略。 Pod与Pod之间的通信，这是测试中的重点，同等条件(物理网卡带宽、CPU等)下性能主要受 CNI 插件影响。 Pod到Service之间的通信，会受到 CNI 与 Kube-proxy 的共同影响，因此也是测试的重点之一。 集群外部与内部组件之间的通信。集群外部访问的模式较多，链路较长，本文不做重点介绍。  除以上场景外，还需要考虑 POD 之间通讯是在同 NODE 与不同 NODE 等两种情况。如果再加上网络协议类型，那么我们需要考虑：通讯场景、协议、跨主机等三个维度的组合。以最常见的模式，我们可以得到以下8种组合：</description>
    </item>
    
    <item>
      <title>Kubernetes 源码调试</title>
      <link>/post/kubernetes-debug-kubernetes-source-code/</link>
      <pubDate>Sun, 17 Jan 2021 12:41:52 +0000</pubDate>
      
      <guid>/post/kubernetes-debug-kubernetes-source-code/</guid>
      <description>前言 本文提供了一种基于 vscode 和 minikube 的快速搭建 Kubernetes 开发调试环境的方法。通过 devle 对 Kubernetes 各个组件进行远程调试。调试代码可以帮助你更深入地理解 Kubernetes 逻辑，更清晰的分析代码执行流程。
准备阶段  Clone kubernetes 源代码 参考官方文档，将 Kubernetes clone 到默认 $GOPATH (Ubuntu 默认路径: ~/go)下  mkdir -p $GOPATH/src/k8s.io cd $GOPATH/src/k8s.io git clone https://github.com/kubernetes/kubernetes cd kubernetes # 以 1.19.x 分支为例我们签出源代码 git checkout release-1.19 # 或签出指定 tag 版本 git checkout v1.</description>
    </item>
    
    <item>
      <title>Kiosk 多租户扩展-基本概念</title>
      <link>/post/kubernetes-multi-tenancy-kiosk/</link>
      <pubDate>Wed, 30 Dec 2020 20:46:17 +0800</pubDate>
      
      <guid>/post/kubernetes-multi-tenancy-kiosk/</guid>
      <description>Kubernetes 多租户扩展  使用 Accounts 与 Account User 在共享的 Kubernetes 集群中区分租户 租户自助 Namespace 空间配置 Account Limits 账户配额保障服务质量与公平性 Namespace Templates 命名空间模板保证租户隔离与自助初始化 Namespace。 *多集群租户管理以共享集群池  Why kiosk Kubernetes 被设计为一个单租户平台，因此在单集群上实现多租户功能变得十分困难。然而，共享一个集群有很多优势，例如，资源利用率高，管理配置成本低，也可使租户更容易共享集群内部资源。
当然，创建多租户 Kubernetes 集群的方法有很多，许多 Kubernetes 发行版也提出了他们自己的租户逻辑，然而却没有一个种轻量级的，可热拔插，并可定制化的解决方案。使管理员可以在任何标准的 Kubernetes 集群之上增加多租户能力。
kiosk 愿景  100% 开源：使用 CNCF 兼容的 Apache 2.0 license 即插即用：可轻而易举地在任何已有集群中安装，适用于多种不同的场景 快速：着重于租户的自动化与自助服务 安全：为不同级别的租户隔离提供默认配置 可扩展：providing building blocks for higher-level Kubernetes platforms  架构 kiosk 的核心理念是使用 Kubernetes Namespace 作为隔离的工作空间，租户的应用可以运行在彼此隔离的空间中。为了降低管理成本，集群管理员应该配置 kiosk，然后它将成为一个租户可以在 Kubernetes 中自助配置 Namespace 的系统。</description>
    </item>
    
    <item>
      <title>Kubernetes 层级命名空间 HNC - 3 Why</title>
      <link>/post/kubernetes-multi-tenancy-hnc/</link>
      <pubDate>Mon, 28 Dec 2020 20:34:37 +0800</pubDate>
      
      <guid>/post/kubernetes-multi-tenancy-hnc/</guid>
      <description>为什么使用 namespaces? 在深入讨论层级空间前，我们有必要先考虑一下为何 Kubernetes 使用了命名空间的概念。
首先，也是最重要的，命名空间提供了一种组织 Kubernetes 对象的方式，可以防止同名问题，即集群中的任何 Kind 的实例对象命名必须唯一的问题。对于不同的 Kind 对象命名是可以相同的，但是同一种对象在同一命名空间下是不允许重现重名。对于 Kubernetes 用户，这也有利于用户使用简短的命名，如 frontend 或 database, 而不必担心在集群中产生命名冲突。
其次，重要也很巧妙的设计是，在控制面中命名空间是安全隔离的主要单元和身份标示。他们具有排他性，也就是说，命名空间中的对象属于且仅属于一个命名空间。如上所述，它将对象进行分组。这些分组的独立性使他们可以自然的关联一组相关的对象。 例如，命名空间被作为各种策略的默认目标，如 RoleBinding, Network Policies, 这也适用于 Kubernetes 扩展，如 Istio Polices。
当然，在命名空间内应用策略也是可行的，但是 Kubernetes 对他的支持有限，且容易引起错误，同时在 Kuberenets 生态系统中容易使用户产生困惑。例如，RBAC 策略可以应用于目标对象的名称，但是这些策略很难以维护。
再如，命名空间中的 Pod 可以使用同空间下的任何 Service Account 运行， 甚至是远超过 Pod 运行所需的特权。管理员在不借助 Validting Admission Contoller 或 OPA Gatekeeper 等高级技术情况下，通常无法直接限制。结果是，作用于 Service Account 的各种策略是弱策略，除非这些 Service Account 来自不同的命名空间。</description>
    </item>
    
    <item>
      <title>使用 apt 命令获取 Ubuntu 安装包源码</title>
      <link>/post/how-to-get-source-code-of-package/</link>
      <pubDate>Mon, 14 Dec 2020 15:03:27 +0000</pubDate>
      
      <guid>/post/how-to-get-source-code-of-package/</guid>
      <description>&lt;p&gt;由于最近需要研究一个关于 Ubuntu 上 systemd 的问题，因此需要下载 systemd 的源代码。 一般源码下载可以在官网或者Github中下载，如 systemd。但是由于 linux 各个分发版本可能会给它们打上自己的补丁，因此我们需要从 Ubuntu 下载其源码包。在 Debian 或 Ubuntu 的系统中源码是文件版的软件发行包，因此我们可以使用 apt-get 或者 apt 命令下载其源码 (DEB 文件包)。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Kubernetes - &#39;No Podsandbox Found with Id&#39; 导致 Kubelet 启动失败分析过程总结</title>
      <link>/post/kubernetes-no-podsandbox-found/</link>
      <pubDate>Tue, 08 Dec 2020 03:38:37 +0000</pubDate>
      
      <guid>/post/kubernetes-no-podsandbox-found/</guid>
      <description>&lt;p&gt;某线上客户反应 Kubernetes 集群中一节点显示为 &amp;ldquo;NotReady&amp;rdquo; 状态，重启 Kubelet 后任然无法恢复。由于无法连接环境，只能拿到少量相关日志信息，因此尝试本地重现。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Kubernetes RBAC - 证书认证</title>
      <link>/post/kubernetes-rbac-certificates/</link>
      <pubDate>Fri, 04 Dec 2020 00:37:34 +0000</pubDate>
      
      <guid>/post/kubernetes-rbac-certificates/</guid>
      <description>X509 客户证书 一般 Kubernetes 集群都会启用基于客户证书的用户认证模式。 API Server 通过 --client-ca-file=SOMEFILE 选项配置 CA 证书用于客户证书认证。如果提供的证书被验证通过，则 subject 中的公共名称（Common Name）就被作为请求的用户名。 通常可以使用两种方式签发：
 通过 CertificateSigningRequest 资源类型允许客户申请签名 X.509 证书 使用外部证书服务颁发证书，如 openssl  通过 CSR API 签发证书步骤：  生成用户私钥：  openssl genrsa -out user2.key 2048 使用私钥生成证书签名请求 CSR:  openssl req -new -key user2.key -out user2.csr -subj &amp;#34;/CN=user2/O=group1/O=group2&amp;#34;  如果遇到错误信息 Can&#39;t load /home/ubuntu/.</description>
    </item>
    
    <item>
      <title>CKA 实战 第二章 安装与升级</title>
      <link>/post/cka-in-action-chapter1-installation/</link>
      <pubDate>Thu, 19 Nov 2020 09:30:43 +0000</pubDate>
      
      <guid>/post/cka-in-action-chapter1-installation/</guid>
      <description>Kubernetes 有多种部署方式，学习环境可以选用 Minikube 或 Kind 等。但在 CKA 认证考试中会考察基于 Kubeadm 的部署方式， 操作系统为 Ubuntu。你可以使用云主机或者VirlualBox虚拟机环境安装 Ubuntu。这里我们推荐在线教学环境 KataKoda。 Kadakda 的课程中提供了交互式教学环境，我们既可以参考其实验步骤，也可独立使用其在线环境。
什么是 Kubeadm? Kubeadm 是 Kubernetes 集群快速安装部署工具，它解决了 Kubernetes 核心组件安装，TLS 加密配置，证书管理等问题，提供了开箱即用的安全特性。
Kubeadm 常用命令  kubeadm init 用于初始化 Kubernetes 控制面 kubeadm join 将工作节点加入集群或者添加其他控制面节点 kubeadm upgrade 用户升级 Kubernetes 集群 kubeadm reset 重置当前主机，恢复 kubeadm init 或 kubeadm join 的配置。  Kubernetes 组件 Kubernetes 集群由控制面与集群节点两部分组成， 节点上运行应用的工作负载 Pod，控制面负责集群中的工作节点与 Pod 的管理。</description>
    </item>
    
    <item>
      <title>Kubernetes Issue 归类指南</title>
      <link>/post/issue-triage/</link>
      <pubDate>Fri, 06 Nov 2020 08:42:33 +0000</pubDate>
      
      <guid>/post/issue-triage/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Issue 管理是软件工程中重要的一个环节。项目管理者可以通过 issue 对项目做持续的追踪，复审，把握整体的质量和进度。同时及时发现风险和问题。在开源软管理过程中，issue 的追踪变得更为棘手，来自全世界开发者的 issue 会源源不断的进入 issue 管理系统中。因此更需要一个高效的管理流程及工具。&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>基于 KubeSphere 的 Spring Could 微服务 CI/CD 实践</title>
      <link>/post/spring-cloud-on-kubeshpere/</link>
      <pubDate>Thu, 22 Oct 2020 02:06:46 +0000</pubDate>
      
      <guid>/post/spring-cloud-on-kubeshpere/</guid>
      <description>&lt;p&gt;本文以 Pig 为例介绍如何在 KubeSphere 上发布一个基于 Spring Cloud 微服务的 CI/CD 项目。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>基于Ubuntu18.04的云原生应用开发环境搭建 – KinD 搭建Kubernetes多节点集群</title>
      <link>/post/kind-setup/</link>
      <pubDate>Mon, 19 Oct 2020 01:13:57 +0000</pubDate>
      
      <guid>/post/kind-setup/</guid>
      <description>基于Ubuntu18.04的云原生应用开发环境搭建 &amp;ndash; KinD 搭建Kubernetes多节点集群 KinD是Kubernetes in Docker的简称，它的主要目标是用于测试Kubernetes本身。也可以用于搭建本地Kubernetes开发环境，开发云原生应用。KinD可以运行在Windows，Macos和linux中。本文以Ubuntu为例，介绍kind的常用命令以及配置。
前提条件  已安装Ubuntu18.04桌面版 已安装Docker  下载最新版KinD 当前kind版本为v0.9.0,可以使用以下命令下载并安装
curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.9.0/kind-linux-amd64 chmod +x ./kind sudo mv ./kind /usr/local/bin/ Kind使用 创建集群 创建Kubernetes集群最简单的方式就是使用kind create cluster命令。执行后3-5分钟即可启动一个K8S集群。同时kind会更新${HOME}/.kube/config下的kubeconfig。默认集群名字为kind，也可通过&amp;ndash;name 指定名称。
常用命令  查看已创建的集群  kind get clusters  删除集群  kind delete cluster --name your-cluster 高级选项 多节点集群 kind最重要的功能之一是创建多节点集群，为了创建多个node，我们需要先创建一个yaml配置文件。如：
# three node (two workers) cluster config kind: Cluster apiVersion: kind.</description>
    </item>
    
    <item>
      <title>基于Ubuntu18.04的云原生应用开发环境搭建VSCode&amp;Golang</title>
      <link>/post/vscode-golang/</link>
      <pubDate>Tue, 22 Sep 2020 01:42:38 +0000</pubDate>
      
      <guid>/post/vscode-golang/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>