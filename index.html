<!DOCTYPE html>
<html lang="zh-cn">
    <head>
	<meta name="generator" content="Hugo 0.74.3" />
		
		
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
			<meta name="description" content="奔四程序员，不会写博客～～">

		<title>Roland&#39;s Blog</title>

		
		<link rel="stylesheet" href="/css/style.css">
		<link rel="stylesheet" href="/css/fonts.css">
		
		<link rel="icon" href="favicon.ico" />
		<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
		<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

		
		<link href="/index.xml" rel="alternate" type="application/rss+xml" title="Roland&#39;s Blog" />
	</head>

    <body>
        		<nav class="nav">
			<div class="nav-container">
				<a href="/">
					
						<h1 class="nav-title">Roland&#39;s Blog</h1>
					
				</a>
				<ul>
    
    
</ul>
			</div>
		</nav>

        

<main>
	


	<div class="catalogue">
		
			<a href="/post/sysctl-kubernetes/" class="catalogue-item">
    <div>
        <time datetime="2022-01-17 14:15:42 &#43;0000 UTC" class="catalogue-time">January 17, 2022</time>
        <h2 class="catalogue-title">Kubernetes 运维必知的内核参数和优化项</h2>
        <div class="catalogue-line"></div>

        <p>
            在 Kubernetes 安装过程中，除必要的内核参数外，Kubernetes 并未提供更多的优化建议。但无论 CentOS 或 Ubuntu 默认内核参数通常并不能满足高负载环境下的性能要求。因此，我们需要调整内核参数以优化节点的性能，并使节点更加稳定与健壮。
内核&amp;内存&amp;文件系统 PID Max  kernel.pid_max = 32768 配置决定了系统中可用的 PID 数量。进程 ID (PID) 是 Linux 内核的一项基础资源，内核会为每个进程或线程分配一个 PID。pid_max 的默认值一般是由内核编译项或 CPU 数量决定的，对于 64位系统内核代码中给出的计算方法如下：  # https://github.com/torvalds/linux/blob/5147da902e0dd162c6254a61e4c57f21b60a9b1c/kernel/pid.c#L657 pid_max = min(4194304, max(32768, 1024 * (number of CPUs))) 以上表达式可以简单理解为，如果小于32核心则默认最大PID值为32768，超过32核心的则按1024* CPUs计算。因此，1024 也是内核推荐的每核心所调度的进程数量。
在一个 Kebenetes 的工作节点即使调度了110个pod， 那么每个 Pod 也可以获得 200 多个 PID。这个限制对大多数应用通常是足够的。但 Kuberentes 集群中通常会存在资源超卖的情况，此时可以适当提高此设置。
        </p>
    </div>
</a>

		
			<a href="/post/kubernetes-webhook-debug-locally/" class="catalogue-item">
    <div>
        <time datetime="2021-12-06 01:13:57 &#43;0000 UTC" class="catalogue-time">December 6, 2021</time>
        <h2 class="catalogue-title">Kubernetes webhook 本地调试方法</h2>
        <div class="catalogue-line"></div>

        <p>
            在 Kubernetes Webhook 开发过程中，调试是复杂场景下不可或缺的步骤。 但 Kubebuilder 文档中只给出了的远程部署运行步骤以及禁用 webhook 的方法，并没有给出本地调试的具体方法，只是了了数字带过：
 If you want to run the webhooks locally, you’ll have to generate certificates for serving the webhooks, and place them in the right directory (/tmp/k8s-webhook-server/serving-certs/tls.{crt,key}, by default).
  If you’re not running a local API server, you’ll also need to figure out how to proxy traffic from the remote cluster to your local webhook server.
        </p>
    </div>
</a>

		
			<a href="/post/kubesphere-routing/" class="catalogue-item">
    <div>
        <time datetime="2021-11-19 13:23:25 &#43;0000 UTC" class="catalogue-time">November 19, 2021</time>
        <h2 class="catalogue-title">深入浅出 Kubernetes 项目网关与应用路由</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>KubeSphere 项目网关与应用路由提供了一种聚合服务的方式，将集群的内部服务通过一个外部可访问的 IP 地址以 HTTP 或 HTTPs 暴露给集群外部。应用路由定义了这些服务的访问规则，用户可以定义基于 host 主机名称和 URL 匹配的规则。同时还可以配置 HTTPs offloading 等选项。项目网关则是应用路由的具体实现，它承载了流量的入口并根据应用路由规则将匹配到的请求转发至集群内的服务。</p>
        </p>
    </div>
</a>

		
			<a href="/post/helm-tips/" class="catalogue-item">
    <div>
        <time datetime="2021-05-24 06:52:16 &#43;0000 UTC" class="catalogue-time">May 24, 2021</time>
        <h2 class="catalogue-title">Helm Tips</h2>
        <div class="catalogue-line"></div>

        <p>
            Helm 是最常用的 Kubernetes 包管理器之一。但实际应使用时却会面对各种复杂的问题，以下内容是对常见应用场景的简要总结：
将 kubectl 部署的资源转为 Helm 资源 如果您的项目最初不是使用 Helm 进行资源管理且已经部署在线上运行，那么你可能需要将已有资源转换 Helm 资源。 否则直接运行 helm install 这时你将看见如下错误信息：
Error: rendered manifests contain a resource that already exists. Unable to continue with install: 
解决这个问题的方法很简单，只需要将对应的资源打上 Helm 的 Label 和 Annotation 即可:
KIND=deployment NAME=my-app-staging RELEASE=staging NAMESPACE=default kubectl annotate $KIND $NAME meta.helm.sh/release-name=$RELEASE kubectl annotate $KIND $NAME meta.
        </p>
    </div>
</a>

		
			<a href="/post/kubernetes-cni-performance/" class="catalogue-item">
    <div>
        <time datetime="2021-04-07 01:13:57 &#43;0000 UTC" class="catalogue-time">April 7, 2021</time>
        <h2 class="catalogue-title">Kubernetes CNI 插件性能测试</h2>
        <div class="catalogue-line"></div>

        <p>
            网络性能是影响 Kubernetes 集群与应用性能的重要指标之一。同时网络架构也是 Kubernetes 中较为复杂模块之一，Kubernetes 通过开放 CNI 接口，实现了网络插件即插即用功能，CNI 插件的实现方式是影响网络性能的最主要因素。目前主流的 CNI 插件包括：Flannel、Calico、Weave、Canal 和 Cilium 等。除 CNI 插件外，Kube-proxy 运行模式也是影响网络性能的重要因素之一。 Kubernetes 通过 Kube-proxy 提供了集群内负载均衡的能力，其内置方式有 Iptables 和 IPVS 等两种。在不同的场景下，CNI 和 Kube-proxy 都会对网络性能产生显著的影响。本文着重对 Kubernetes 网络性能测试的方法以及影响因素进行讲解和分析，不包含具体插件的性能比较。
Kubernetes 网络通讯模式 在 Kubernetes 网络中，主要有以下几种通讯场景：
 容器与容器之间的直接通信，通过本地回环实现，不通过网络插件协议栈，因此测试可以忽略。 Pod与Pod之间的通信，这是测试中的重点，同等条件(物理网卡带宽、CPU等)下性能主要受 CNI 插件影响。 Pod到Service之间的通信，会受到 CNI 与 Kube-proxy 的共同影响，因此也是测试的重点之一。 集群外部与内部组件之间的通信。集群外部访问的模式较多，链路较长，本文不做重点介绍。  除以上场景外，还需要考虑 POD 之间通讯是在同 NODE 与不同 NODE 等两种情况。如果再加上网络协议类型，那么我们需要考虑：通讯场景、协议、跨主机等三个维度的组合。以最常见的模式，我们可以得到以下8种组合：
        </p>
    </div>
</a>

		
	</div>

	<div class="pagination">
		
		
			<a href="/page/2/" class="right arrow">&#8594;</a>
		

		<span>1</span>
	</div>
</main>


        		<footer>
			
			<span>
			&copy; <time datetime="2022-01-18 07:36:56.99912407 &#43;0000 UTC m=&#43;0.289605799">2022</time> . Made with <a href='https://gohugo.io'>Hugo</a> using the <a href='https://github.com/EmielH/tale-hugo/'>Tale</a> theme.
			</span>
		</footer>

    </body>
</html>
